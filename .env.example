# AI Configuration for CLI RPG
# Copy this file to .env and fill in your values

# ============================================================================
# Provider Selection
# ============================================================================
# Choose your AI provider: openai, anthropic, or ollama
# AI_PROVIDER=openai

# OpenAI Configuration
# OPENAI_API_KEY=your-openai-api-key-here

# Anthropic Configuration
# ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Ollama Configuration (Local AI - no API key required)
# Set AI_PROVIDER=ollama to use local Ollama models
# OLLAMA_BASE_URL=http://localhost:11434/v1
# OLLAMA_MODEL=llama3.2

# ============================================================================
# Optional AI Configuration
# ============================================================================
# Model to use (default depends on provider)
# - OpenAI: gpt-3.5-turbo
# - Anthropic: claude-3-5-sonnet-latest
# - Ollama: llama3.2
AI_MODEL=gpt-3.5-turbo

# Temperature for generation (0.0-2.0, default: 0.7)
AI_TEMPERATURE=0.7

# Maximum tokens in response (default: 500)
AI_MAX_TOKENS=500

# Maximum retry attempts on errors (default: 3)
AI_MAX_RETRIES=3

# Delay between retries in seconds (default: 1.0)
AI_RETRY_DELAY=1.0

# Enable caching (true/false, default: true)
AI_ENABLE_CACHING=true

# Cache TTL in seconds (default: 3600)
AI_CACHE_TTL=3600

# Strict mode for AI generation (true/false, default: true)
# When true, AI failures prompt user with options (retry, fallback, quit)
# When false, silently falls back to default world on AI failure
CLI_RPG_REQUIRE_AI=true
